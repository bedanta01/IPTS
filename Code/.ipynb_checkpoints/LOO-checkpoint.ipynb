{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import codecs\n",
    "from collections import Counter,defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>UserHandle</th>\n",
       "      <th>Party</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a no of people approach me daily worried abt t...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>GST</td>\n",
       "      <td>Disagreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its now revealed that our fms silence on the p...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>PNB Scam</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pnb scam started in is going on till today the...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>PNB Scam</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would bjp confirm this if true what transpired...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>PNB Scam</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bjp insiders telling that niravmodi been a reg...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>PNB Scam</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet      UserHandle Party  \\\n",
       "0  a no of people approach me daily worried abt t...  ArvindKejriwal   AAP   \n",
       "1  its now revealed that our fms silence on the p...  ArvindKejriwal   AAP   \n",
       "2  pnb scam started in is going on till today the...  ArvindKejriwal   AAP   \n",
       "3  would bjp confirm this if true what transpired...  ArvindKejriwal   AAP   \n",
       "4  bjp insiders telling that niravmodi been a reg...  ArvindKejriwal   AAP   \n",
       "\n",
       "      Issue        Stance  \n",
       "0       GST  Disagreement  \n",
       "1  PNB Scam       Neutral  \n",
       "2  PNB Scam       Neutral  \n",
       "3  PNB Scam       Neutral  \n",
       "4  PNB Scam       Neutral  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataframe = pd.read_csv(\"../data/Labelled_tweets_v1.csv\")\n",
    "raw_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agreement' 'Disagreement' 'Neutral']\n",
      "{'Neutral': 2, 'Disagreement': 1, 'Agreement': 0}\n",
      "['Aadhar linking' 'Beef Ban' 'Cauvery SC Verdict' 'Demonetisation'\n",
      " 'EVM tampering' 'FDIPolicy' 'Fodder scam' 'GDP growth' 'GST'\n",
      " 'Inflation control' 'Jallikattu ban' 'PNB Scam'\n",
      " 'Padmavati film screening' 'Ram Mandir' 'RightToPrivacy SC Verdict'\n",
      " 'Rohingyas' 'Swacch Bharat' 'Triple Talaq SC verdict' 'TripleTalaqBill'\n",
      " 'acchedin' 'hike in oil prices' 'lgp price hike' 'nsc and ppf rate cuts'\n",
      " 'reservation']\n",
      "{'nsc and ppf rate cuts': 22, 'Rohingyas': 15, 'Aadhar linking': 0, 'Inflation control': 9, 'Demonetisation': 3, 'EVM tampering': 4, 'Jallikattu ban': 10, 'FDIPolicy': 5, 'Fodder scam': 6, 'hike in oil prices': 20, 'Triple Talaq SC verdict': 17, 'GDP growth': 7, 'lgp price hike': 21, 'Ram Mandir': 13, 'Swacch Bharat': 16, 'GST': 8, 'Cauvery SC Verdict': 2, 'acchedin': 19, 'reservation': 23, 'RightToPrivacy SC Verdict': 14, 'Padmavati film screening': 12, 'PNB Scam': 11, 'Beef Ban': 1, 'TripleTalaqBill': 18}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>UserHandle</th>\n",
       "      <th>Party</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a no of people approach me daily worried abt t...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its now revealed that our fms silence on the p...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pnb scam started in is going on till today the...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would bjp confirm this if true what transpired...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bjp insiders telling that niravmodi been a reg...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is it possible to believe that he or vijay mal...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>someone told me today sealing bjp wants to rui...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>if all state govts central govt and sc togethe...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>three killings on merchants in one year first ...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>there is no need to increase house tax in mcd ...</td>\n",
       "      <td>ArvindKejriwal</td>\n",
       "      <td>AAP</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet      UserHandle Party  \\\n",
       "0  a no of people approach me daily worried abt t...  ArvindKejriwal   AAP   \n",
       "1  its now revealed that our fms silence on the p...  ArvindKejriwal   AAP   \n",
       "2  pnb scam started in is going on till today the...  ArvindKejriwal   AAP   \n",
       "3  would bjp confirm this if true what transpired...  ArvindKejriwal   AAP   \n",
       "4  bjp insiders telling that niravmodi been a reg...  ArvindKejriwal   AAP   \n",
       "5  is it possible to believe that he or vijay mal...  ArvindKejriwal   AAP   \n",
       "6  someone told me today sealing bjp wants to rui...  ArvindKejriwal   AAP   \n",
       "7  if all state govts central govt and sc togethe...  ArvindKejriwal   AAP   \n",
       "8  three killings on merchants in one year first ...  ArvindKejriwal   AAP   \n",
       "9  there is no need to increase house tax in mcd ...  ArvindKejriwal   AAP   \n",
       "\n",
       "   Issue  Stance  \n",
       "0      8       1  \n",
       "1     11       2  \n",
       "2     11       2  \n",
       "3     11       2  \n",
       "4     11       2  \n",
       "5     11       2  \n",
       "6      5       2  \n",
       "7      5       2  \n",
       "8      8       1  \n",
       "9      8       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index on classes\n",
    "stance_df = raw_dataframe.Stance\n",
    "stances = np.unique(stance_df)\n",
    "print(stances)\n",
    "stance_idx = {}\n",
    "for i in range(len(stances)):\n",
    "    stance_idx[stances[i]] = i\n",
    "print(stance_idx)\n",
    "raw_dataframe.Stance.replace(to_replace = stance_idx, inplace = True)\n",
    "\n",
    "issue_df = raw_dataframe['Issue']\n",
    "issues = np.unique(issue_df)\n",
    "print(issues)\n",
    "issue_idx = {}\n",
    "for i in range(len(issues)):\n",
    "    issue_idx[issues[i]] = i\n",
    "print(issue_idx)\n",
    "raw_dataframe.Issue.replace(to_replace=issue_idx, inplace=True)\n",
    "raw_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove selected stopwords from the dataset\n",
    "stop_words = ['the','of','in','and','a','is','on','this','all','it','will','for','to','be','with',\n",
    "              'at','are','u','has','that','by','from', 'as','was','have','its','an','if','been','be','also','should','which']\n",
    "for count, row in raw_dataframe.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word not in stop_words:\n",
    "            new_tweet.append(word)\n",
    "    raw_dataframe.iloc[count, 0] = ' '.join(new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_IDX = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, input_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, self.input_size, padding_idx=PADDING_IDX)\n",
    "        self.GRU = nn.LSTM(self.input_size, self.hidden_size,num_layers = 1, batch_first = True)\n",
    "        self.linear = nn.Linear(self.hidden_size, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x_input,hidden_state):\n",
    "        embedded = self.word_embeddings(x_input)\n",
    "        #print(embedded.shape)\n",
    "        #output = embedded.view(-1,-1, self.hidden_size)\n",
    "        output, hn = self.GRU(embedded)\n",
    "        #hn_ = hn[0].reshape(x_input.shape[0],output.shape[1]*output.shape[2])\n",
    "        #print(output.shape)\n",
    "        linear = self.linear(hn[0])\n",
    "        #print(linear.shape)\n",
    "        class_labels = self.softmax(linear)\n",
    "        \n",
    "        return class_labels\n",
    "\n",
    "    def H_t0(self, batch_size):\n",
    "        return torch.zeros(1,batch_size,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out issue: nsc and ppf rate cuts, idx: 22\n",
      "Splitting dataset based on issue\n",
      "(7157,) (7003,) (7157,)\n",
      "(35,) (189,) (35,)\n",
      "Generating tf-idf vectors\n",
      "(7157, 16914)\n",
      "(35, 16914)\n",
      "Stance classification\n",
      "Accuracy:  0.6\n",
      "Leaving out issue: Rohingyas, idx: 15\n",
      "Splitting dataset based on issue\n",
      "(7171,) (7003,) (7171,)\n",
      "(21,) (189,) (21,)\n",
      "Generating tf-idf vectors\n",
      "(7171, 16906)\n",
      "(21, 16906)\n",
      "Stance classification\n",
      "Accuracy:  0.2857142857142857\n",
      "Leaving out issue: Aadhar linking, idx: 0\n",
      "Splitting dataset based on issue\n",
      "(7051,) (7003,) (7051,)\n",
      "(141,) (189,) (141,)\n",
      "Generating tf-idf vectors\n",
      "(7051, 16713)\n",
      "(141, 16713)\n",
      "Stance classification\n",
      "Accuracy:  0.5957446808510638\n",
      "Leaving out issue: Inflation control, idx: 9\n",
      "Splitting dataset based on issue\n",
      "(6979,) (7003,) (6979,)\n",
      "(213,) (189,) (213,)\n",
      "Generating tf-idf vectors\n",
      "(6979, 16692)\n",
      "(213, 16692)\n",
      "Stance classification\n",
      "Accuracy:  0.6009389671361502\n",
      "Leaving out issue: Demonetisation, idx: 3\n",
      "Splitting dataset based on issue\n",
      "(5946,) (7003,) (5946,)\n",
      "(1246,) (189,) (1246,)\n",
      "Generating tf-idf vectors\n",
      "(5946, 15567)\n",
      "(1246, 15567)\n",
      "Stance classification\n",
      "Accuracy:  0.5914927768860353\n",
      "Leaving out issue: EVM tampering, idx: 4\n",
      "Splitting dataset based on issue\n",
      "(7011,) (7003,) (7011,)\n",
      "(181,) (189,) (181,)\n",
      "Generating tf-idf vectors\n",
      "(7011, 16583)\n",
      "(181, 16583)\n",
      "Stance classification\n",
      "Accuracy:  0.39779005524861877\n",
      "Leaving out issue: Jallikattu ban, idx: 10\n",
      "Splitting dataset based on issue\n",
      "(7159,) (7003,) (7159,)\n",
      "(33,) (189,) (33,)\n",
      "Generating tf-idf vectors\n",
      "(7159, 16888)\n",
      "(33, 16888)\n",
      "Stance classification\n",
      "Accuracy:  0.06060606060606061\n",
      "Leaving out issue: FDIPolicy, idx: 5\n",
      "Splitting dataset based on issue\n",
      "(7037,) (7003,) (7037,)\n",
      "(155,) (189,) (155,)\n",
      "Generating tf-idf vectors\n",
      "(7037, 16751)\n",
      "(155, 16751)\n",
      "Stance classification\n",
      "Accuracy:  0.5870967741935483\n",
      "Leaving out issue: Fodder scam, idx: 6\n",
      "Splitting dataset based on issue\n",
      "(7124,) (7003,) (7124,)\n",
      "(68,) (189,) (68,)\n",
      "Generating tf-idf vectors\n",
      "(7124, 16829)\n",
      "(68, 16829)\n",
      "Stance classification\n",
      "Accuracy:  0.75\n",
      "Leaving out issue: hike in oil prices, idx: 20\n",
      "Splitting dataset based on issue\n",
      "(6824,) (7003,) (6824,)\n",
      "(368,) (189,) (368,)\n",
      "Generating tf-idf vectors\n",
      "(6824, 16476)\n",
      "(368, 16476)\n",
      "Stance classification\n",
      "Accuracy:  0.6820652173913043\n",
      "Leaving out issue: Triple Talaq SC verdict, idx: 17\n",
      "Splitting dataset based on issue\n",
      "(7132,) (7003,) (7132,)\n",
      "(60,) (189,) (60,)\n",
      "Generating tf-idf vectors\n",
      "(7132, 16870)\n",
      "(60, 16870)\n",
      "Stance classification\n",
      "Accuracy:  0.8833333333333333\n",
      "Leaving out issue: GDP growth, idx: 7\n",
      "Splitting dataset based on issue\n",
      "(6838,) (7003,) (6838,)\n",
      "(354,) (189,) (354,)\n",
      "Generating tf-idf vectors\n",
      "(6838, 16548)\n",
      "(354, 16548)\n",
      "Stance classification\n",
      "Accuracy:  0.6666666666666666\n",
      "Leaving out issue: lgp price hike, idx: 21\n",
      "Splitting dataset based on issue\n",
      "(6987,) (7003,) (6987,)\n",
      "(205,) (189,) (205,)\n",
      "Generating tf-idf vectors\n",
      "(6987, 16733)\n",
      "(205, 16733)\n",
      "Stance classification\n",
      "Accuracy:  0.8585365853658536\n",
      "Leaving out issue: Ram Mandir, idx: 13\n",
      "Splitting dataset based on issue\n",
      "(6969,) (7003,) (6969,)\n",
      "(223,) (189,) (223,)\n",
      "Generating tf-idf vectors\n",
      "(6969, 16320)\n",
      "(223, 16320)\n",
      "Stance classification\n",
      "Accuracy:  0.5067264573991032\n",
      "Leaving out issue: Swacch Bharat, idx: 16\n",
      "Splitting dataset based on issue\n",
      "(6969,) (7003,) (6969,)\n",
      "(223,) (189,) (223,)\n",
      "Generating tf-idf vectors\n",
      "(6969, 16581)\n",
      "(223, 16581)\n",
      "Stance classification\n",
      "Accuracy:  0.905829596412556\n",
      "Leaving out issue: GST, idx: 8\n",
      "Splitting dataset based on issue\n",
      "(4615,) (7003,) (4615,)\n",
      "(2577,) (189,) (2577,)\n",
      "Generating tf-idf vectors\n",
      "(4615, 13646)\n",
      "(2577, 13646)\n",
      "Stance classification\n",
      "Accuracy:  0.6391152502910361\n",
      "Leaving out issue: Cauvery SC Verdict, idx: 2\n",
      "Splitting dataset based on issue\n",
      "(7026,) (7003,) (7026,)\n",
      "(166,) (189,) (166,)\n",
      "Generating tf-idf vectors\n",
      "(7026, 16727)\n",
      "(166, 16727)\n",
      "Stance classification\n",
      "Accuracy:  0.3855421686746988\n",
      "Leaving out issue: acchedin, idx: 19\n",
      "Splitting dataset based on issue\n",
      "(7108,) (7003,) (7108,)\n",
      "(84,) (189,) (84,)\n",
      "Generating tf-idf vectors\n",
      "(7108, 16836)\n",
      "(84, 16836)\n",
      "Stance classification\n",
      "Accuracy:  0.7738095238095238\n",
      "Leaving out issue: reservation, idx: 23\n",
      "Splitting dataset based on issue\n",
      "(7034,) (7003,) (7034,)\n",
      "(158,) (189,) (158,)\n",
      "Generating tf-idf vectors\n",
      "(7034, 16710)\n",
      "(158, 16710)\n",
      "Stance classification\n",
      "Accuracy:  0.4430379746835443\n",
      "Leaving out issue: RightToPrivacy SC Verdict, idx: 14\n",
      "Splitting dataset based on issue\n",
      "(7156,) (7003,) (7156,)\n",
      "(36,) (189,) (36,)\n",
      "Generating tf-idf vectors\n",
      "(7156, 16914)\n",
      "(36, 16914)\n",
      "Stance classification\n",
      "Accuracy:  0.5277777777777778\n",
      "Leaving out issue: Padmavati film screening, idx: 12\n",
      "Splitting dataset based on issue\n",
      "(7093,) (7003,) (7093,)\n",
      "(99,) (189,) (99,)\n",
      "Generating tf-idf vectors\n",
      "(7093, 16663)\n",
      "(99, 16663)\n",
      "Stance classification\n",
      "Accuracy:  0.5959595959595959\n",
      "Leaving out issue: PNB Scam, idx: 11\n",
      "Splitting dataset based on issue\n",
      "(6963,) (7003,) (6963,)\n",
      "(229,) (189,) (229,)\n",
      "Generating tf-idf vectors\n",
      "(6963, 16407)\n",
      "(229, 16407)\n",
      "Stance classification\n",
      "Accuracy:  0.537117903930131\n",
      "Leaving out issue: Beef Ban, idx: 1\n",
      "Splitting dataset based on issue\n",
      "(7064,) (7003,) (7064,)\n",
      "(128,) (189,) (128,)\n",
      "Generating tf-idf vectors\n",
      "(7064, 16689)\n",
      "(128, 16689)\n",
      "Stance classification\n",
      "Accuracy:  0.5390625\n",
      "Leaving out issue: TripleTalaqBill, idx: 18\n",
      "Splitting dataset based on issue\n",
      "(7003,) (7003,) (7003,)\n",
      "(189,) (189,) (189,)\n",
      "Generating tf-idf vectors\n",
      "(7003, 16720)\n",
      "(189, 16720)\n",
      "Stance classification\n",
      "Accuracy:  0.7777777777777778\n",
      "Avg accuracy:  0.5913225804211945\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for issue, idx in issue_idx.items():\n",
    "    print('Leaving out issue: {0}, idx: {1}'.format(issue, idx))\n",
    "    train_data = raw_dataframe[raw_dataframe.Issue != idx]\n",
    "    test_data = raw_dataframe[raw_dataframe.Issue == idx]\n",
    "    \n",
    "    print('Splitting dataset based on issue')\n",
    "    X_train = train_data['Tweet'].values\n",
    "#     Y1_train = train_data['Issue'].values\n",
    "    Y2_train = train_data['Stance'].values\n",
    "    print(X_train.shape, Y1_train.shape, Y2_train.shape)\n",
    "    \n",
    "    X_test = test_data['Tweet'].values\n",
    "#     Y1_test = test_data['Issue'].values\n",
    "    Y2_test = test_data['Stance'].values\n",
    "    print(X_test.shape, Y1_test.shape, Y2_test.shape)\n",
    "    \n",
    "    print('Generating tf-idf vectors')\n",
    "    # Get tfidf vectors for train, val and test\n",
    "    tfidf_vec = TfidfVectorizer()\n",
    "    X_train_tf = tfidf_vec.fit_transform(X_train).toarray()\n",
    "    print(X_train_tf.shape)\n",
    "    \n",
    "    X_test_tf = tfidf_vec.transform(X_test).toarray()\n",
    "    print(X_test_tf.shape)\n",
    "    \n",
    "    # Classify using Logistic regression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    log2 = LogisticRegression()\n",
    "    y2_pred = log2.fit(X_train_tf, Y2_train).predict(X_test_tf)\n",
    "\n",
    "    print('Stance classification')\n",
    "    acc = accuracy_score(Y2_test, y2_pred)\n",
    "    accuracies.append(acc)\n",
    "    print('Accuracy: ', acc)\n",
    "#     print('Weighted F1 score: ', f1_score(Y2_test, y2_pred, averag÷e='weighted'))\n",
    "accuracies = np.array(accuracies)\n",
    "print('Avg accuracy: ', np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length:  17539\n"
     ]
    }
   ],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def getSentIdx(self, sentence):\n",
    "        idx = []\n",
    "        for word in sentence.split(' '):\n",
    "            idx.append(self.word2index.get(word, 1))\n",
    "        return idx\n",
    "            \n",
    "raw_dataframe['Tweet_idx'] = 0\n",
    "voc = Voc('stance')     \n",
    "for count, row in raw_dataframe.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "    voc.addSentence(tweet)\n",
    "    raw_dataframe.iloc[count, 5] = str(voc.getSentIdx(tweet))\n",
    "    \n",
    "voc_len = len(voc.word2index) + 100\n",
    "print('Vocab length: ', voc_len)\n",
    "raw_dataframe.to_csv('../Dataset/Dataset_with_vocidx.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_PER_DOC = 60\n",
    "PADDING_IDX = 1\n",
    "class StanceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        super(StanceDataset, self).__init__()\n",
    "        self.ds = df\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        content = self.ds.iloc[idx, 5]\n",
    "        content = literal_eval(content)\n",
    "        if len(content) >= WORDS_PER_DOC:\n",
    "            content = np.array(content[:WORDS_PER_DOC])\n",
    "        else:\n",
    "            content = np.pad(content, (0, WORDS_PER_DOC-len(content)), 'constant', constant_values=(0, PADDING_IDX))\n",
    "        label = int(self.ds.iloc[idx, 4])\n",
    "#         print(content)\n",
    "        return torch.from_numpy(content), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(loader):\n",
    "    running_loss = 0\n",
    "    num_batches = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        # get the inputs\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        h_0 = Variable(torch.zeros(1, inputs.shape[0], 256))\n",
    "        c_0 = Variable(torch.zeros(1, inputs.shape[0], 256))\n",
    "        outputs = classifierNet(inputs, h_0, c_0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        num_batches += 1\n",
    "    \n",
    "    return running_loss/num_batches, 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out issue: nsc and ppf rate cuts, idx: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bedanta/ml/lib/python3.5/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/bedanta/ml/lib/python3.5/site-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0971)\n",
      "tensor(1.0912)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = []\n",
    "for issue, idx in issue_idx.items():\n",
    "    print('Leaving out issue: {0}, idx: {1}'.format(issue, idx))\n",
    "    train_data = raw_dataframe[raw_dataframe.Issue != idx]\n",
    "    test_data = raw_dataframe[raw_dataframe.Issue == idx]\n",
    "    \n",
    "    train_ds = StanceDataset(train_data)\n",
    "    test_ds = StanceDataset(test_data)\n",
    "    \n",
    "    # Create loader instances\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(dataset = train_ds, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_ds, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "    net = Net(voc_len, 256, 100)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adagrad(net.parameters(),lr=0.1)\n",
    "    epochs = 2\n",
    "    epoch_losses = []\n",
    "    for e in range(epochs):\n",
    "        batch = 1\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            batch_x, batch_y = data\n",
    "#             print(batch_x)\n",
    "            batch_x = Variable(torch.LongTensor(batch_x))\n",
    "            batch_y = Variable(torch.LongTensor(batch_y))\n",
    "            batch_y = batch_y.reshape(batch_y.shape[0])\n",
    "            encoder_hidden = Variable(net.H_t0(batch_y.shape[0]))\n",
    "            output = net(batch_x,encoder_hidden)\n",
    "            loss = criterion(output[0], batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "            running_loss += loss.data[0]\n",
    "            if batch%30==0:\n",
    "                epoch_losses.append(running_loss/30)\n",
    "                running_loss = 0\n",
    "            batch+=1\n",
    "        epoch_loss /= len(train_data)/batch_size\n",
    "        print(epoch_loss)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "#     encoder_hidden_test = Variable(net.H_t0(len(x_tweet_test)))\n",
    "#     predicted_output = net(x_tweet_test,encoder_hidden_test)\n",
    "#     predicted_output = predicted_output.reshape(len(x_tweet_test),3)\n",
    "#     predicted_output = predicted_output.detach().numpy()\n",
    "#     predicted_output = np.argmax(predicted_output,axis = 1)\n",
    "#     predicted_list[one] = predicted_output\n",
    "#     ground_truth[one] = y_label_test\n",
    "#     accuracy = accuracy_score(predicted_output,y_label_test)\n",
    "#     print(accuracy)\n",
    "    plt.plot(epoch_losses)\n",
    "    plt.show()\n",
    "#     epoch_loss_all_issues[one] = epoch_losses\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
